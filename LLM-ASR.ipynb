{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7971457",
   "metadata": {},
   "source": [
    "### 背景音人声分离（三种方法选择一种就行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42101144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载UVR模型\n",
    "from uvr.utils.get_models import download_all_models\n",
    "import json\n",
    "\n",
    "models_json = json.load(open(\"./uvr_models.json\", \"r\"))\n",
    "download_all_models(models_json)#, save_path=\"./uvr_model\")\n",
    "# 模型会默认下载到Conda虚拟环境的uvr目录下，最好不要随便更改下载目录，这个项目的模型载入非常神经"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cbd7d",
   "metadata": {},
   "source": [
    "##### MDX人声提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDX人声提取，速度相对较慢\n",
    "from uvr import models\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "audio_file = f\"E:/AI/ASR/kotoba/DA3.wav\"\n",
    "\n",
    "init_other_metadata = {\n",
    "            'segment_size': 256,\n",
    "            'overlap': 0.75,\n",
    "            'mdx_batch_size': 1,\n",
    "            'semitone_shift': 0,\n",
    "            'adjust': 1.08, \n",
    "            'denoise': False,\n",
    "            'is_invert_spec': False,\n",
    "            'is_match_frequency_pitch': True,\n",
    "            'overlap_mdx': None\n",
    "        }\n",
    "'''\n",
    "{'segment_size': 256}\n",
    "'''\n",
    "\n",
    "mdx = models.MDX(name=\"UVR-MDX-NET-Inst_HQ_4\", other_metadata=init_other_metadata, logger=None)\n",
    "# 可以使用MDX23C-8KFFT-InstVoc_HQ，专门用来分离人声的模型，模型更大，理论上效果更好\n",
    "\n",
    "res = mdx(audio_file)\n",
    "vocals = res[\"vocals\"]\n",
    "\n",
    "# 输出人声提取结果\n",
    "vocals = res[\"vocals\"].T # MDX模型的结果需要转置\n",
    "\n",
    "uvr_sample_rate = 44100 \n",
    "# MDX模型的采样率为44100\n",
    "# 参见https://github.com/jhj0517/ultimatevocalremover_api/blob/3543be1349ce601568787b69cca0c1f8acba7c2e/src/models.py#L294\n",
    "\n",
    "uvr_result = f\"./temp/uvr_result.wav\"\n",
    "sf.write(uvr_result, vocals, uvr_sample_rate, format=\"WAV\")\n",
    "\n",
    "# 删除模型对象本身\n",
    "mdx.model_run = None\n",
    "mdx = None\n",
    "\n",
    "# 建议Python进行垃圾回收\n",
    "gc.collect()\n",
    "\n",
    "# 清理PyTorch在GPU上缓存的但未被使用的内存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449181c0",
   "metadata": {},
   "source": [
    "##### Demucs人声提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a697768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demucs人声提取，基于ultimatevocalremover_api，速度快一点\n",
    "import torch\n",
    "from uvr import models\n",
    "import soundfile as sf\n",
    "\n",
    "audio_file = f\"E:/AI/ASR/kotoba/DA3.wav\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "demucs = models.Demucs(name=\"hdemucs_mmi\", other_metadata={\"segment\":2, \"split\":True}, device=device, logger=None)\n",
    "\n",
    "res = demucs(audio_file)\n",
    "vocals = res[\"vocals\"]\n",
    "\n",
    "# Demucs 模型需要将torch.tensor转换为numpy数组，然后转置\n",
    "vocals_numpy = vocals.detach().cpu().numpy()\n",
    "vocals_numpy = vocals_numpy.T\n",
    "\n",
    "uvr_sample_rate = 44100\n",
    "uvr_result = f\"./temp/uvr_result.wav\"\n",
    "sf.write(uvr_result, vocals_numpy, uvr_sample_rate, format=\"WAV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187736c0",
   "metadata": {},
   "source": [
    "##### HT Demucs ft (v4)人声提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HT Demucs ft (v4)，基于facebookresearch/demucs，无需额外下载模型\n",
    "import os\n",
    "\n",
    "audio_file = f\"E:/AI/ASR/kotoba/DA3.wav\"\n",
    "full_filename = os.path.basename(audio_file)\n",
    "source_file_name, suffix = os.path.splitext(full_filename)\n",
    "\n",
    "!demucs {audio_file} --two-stems vocals --out \"./temp\"\n",
    "\n",
    "uvr_result = f\"./temp/htdemucs/{source_file_name}/vocals.wav\"\n",
    "HT_Demucs_ft_v4 = True\n",
    "# 采样率为44100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6337c",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95cc9e",
   "metadata": {},
   "source": [
    "### Silero人声活动检测及时间戳提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人声活动检测\n",
    "import torch, gc\n",
    "\n",
    "VAD_SAMPLING_RATE = 16000\n",
    "new_hub_dir = './vad_model' \n",
    "torch.hub.set_dir(new_hub_dir)\n",
    "\n",
    "vad_model, vad_utils = torch.hub.load(\n",
    "    repo_or_dir='snakers4/silero-vad',\n",
    "    model='silero_vad',\n",
    "    # force_reload=False,\n",
    "    force_reload=True, # 每次使用强制下载最新模型\n",
    "    # onnx=False\n",
    ")\n",
    "\n",
    "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = vad_utils\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 1. 读取音频\n",
    "    wav = read_audio(uvr_result, sampling_rate=VAD_SAMPLING_RATE)\n",
    "        \n",
    "    # 2. 获取语音活动时间戳\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        audio=wav,\n",
    "        model=vad_model,  # 直接使用已加载的模型\n",
    "        threshold=0.5,  # speech prob threshold (可根据需要调整)\n",
    "        sampling_rate=VAD_SAMPLING_RATE,\n",
    "        min_speech_duration_ms=250, #250,  # min speech duration in ms\n",
    "        max_speech_duration_s=30,\n",
    "        min_silence_duration_ms=1*1000, #100, # 1.5s ~ 2s\n",
    "        speech_pad_ms=200, #30,  # spech pad ms\n",
    "        window_size_samples=512,  # window size\n",
    "    )\n",
    "\n",
    "# 卸载模型\n",
    "vad_model = None\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并连续的短音频块\n",
    "\n",
    "def merge_short_segments(\n",
    "    speech_timestamps: list[dict], \n",
    "    sampling_rate: int,\n",
    "    max_duration_s: float = 10.0,\n",
    "    max_gap_s: float = 1.0\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Merges consecutive short audio segments into longer ones.\n",
    "\n",
    "    Args:\n",
    "        speech_timestamps (list[dict]): \n",
    "            The list of speech segments, e.g., [{'start': ..., 'end': ...}].\n",
    "        sampling_rate (int): \n",
    "            The sampling rate of the audio (e.g., 16000).\n",
    "        max_duration_s (float): \n",
    "            The duration in seconds. Segments shorter than this will be\n",
    "            considered for merging. Defaults to 10.0.\n",
    "        max_gap_s (float): \n",
    "            The maximum allowed silence gap in seconds between two segments\n",
    "            to consider them \"consecutive\". Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A new list of merged speech segments.\n",
    "    \"\"\"\n",
    "    if not speech_timestamps:\n",
    "        return []\n",
    "\n",
    "    # 将秒转换为样本数，便于计算\n",
    "    max_duration_samples = max_duration_s * sampling_rate\n",
    "    max_gap_samples = max_gap_s * sampling_rate\n",
    "\n",
    "    merged_segments = []\n",
    "    \n",
    "    # 将第一个片段作为当前合并块的起点\n",
    "    current_merge_block = speech_timestamps[0].copy()\n",
    "\n",
    "    for next_segment in speech_timestamps[1:]:\n",
    "        \n",
    "        # 计算当前合并块和下一个片段的持续时间\n",
    "        current_block_duration = current_merge_block['end'] - current_merge_block['start']\n",
    "        next_segment_duration = next_segment['end'] - next_segment['start']\n",
    "        \n",
    "        # 计算它们之间的静默间隔\n",
    "        gap_between = next_segment['start'] - current_merge_block['end']\n",
    "\n",
    "        # 检查是否满足所有合并条件\n",
    "        if (current_block_duration < max_duration_samples and\n",
    "            next_segment_duration < max_duration_samples and\n",
    "            gap_between <= max_gap_samples):\n",
    "            \n",
    "            # --- 合并 ---\n",
    "            # 扩展当前块的结束点，以包含下一个片段\n",
    "            current_merge_block['end'] = next_segment['end']\n",
    "        else:\n",
    "            # --- 不合并 ---\n",
    "            # 1. 将已完成的当前块存入结果列表\n",
    "            merged_segments.append(current_merge_block)\n",
    "            # 2. 将下一个片段作为新的合并块起点\n",
    "            current_merge_block = next_segment.copy()\n",
    "\n",
    "    # 循环结束后，不要忘记添加最后一个正在处理的块\n",
    "    merged_segments.append(current_merge_block)\n",
    "\n",
    "    return merged_segments\n",
    "\n",
    "merged_segments = merge_short_segments(\n",
    "    speech_timestamps=speech_timestamps,\n",
    "    sampling_rate=VAD_SAMPLING_RATE,\n",
    "    max_duration_s=5,\n",
    "    max_gap_s=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "def split_audio_by_vad(timestamps, audio_path, vad_sr, out_dir):\n",
    "    \"\"\"\n",
    "    根据VAD时间戳列表切分原始音频文件。\n",
    "\n",
    "    Args:\n",
    "        timestamps (list): VAD输出的时间戳字典列表。\n",
    "        audio_path (str): 原始音频文件的路径。\n",
    "        vad_sr (int): VAD分析时使用的采样率。\n",
    "        out_dir (str): 保存切片文件的目录。\n",
    "    \"\"\"\n",
    "    # 确保输出目录存在\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        print(f\"创建输出目录: {out_dir}\")\n",
    "\n",
    "    # 获取原始音频文件的信息\n",
    "    try:\n",
    "        info = sf.info(audio_path)\n",
    "        original_sr = info.samplerate\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 无法读取音频文件 {audio_path}。 {e}\")\n",
    "        return\n",
    "\n",
    "    # 遍历VAD时间戳列表\n",
    "    for i, ts in enumerate(timestamps):\n",
    "        # VAD 给出的采样点索引\n",
    "        vad_start_sample = ts['start']\n",
    "        vad_end_sample = ts['end']\n",
    "\n",
    "        # 将 VAD 的采样点索引转换为时间（秒）\n",
    "        start_time_s = vad_start_sample / vad_sr\n",
    "        end_time_s = vad_end_sample / vad_sr\n",
    "\n",
    "        # 将时间（秒）转换为原始文件的采样点索引\n",
    "        original_start_sample = int(start_time_s * original_sr)\n",
    "        original_end_sample = int(end_time_s * original_sr)\n",
    "        \n",
    "        # 使用 soundfile 从原始文件中读取精确的音频块\n",
    "        try:\n",
    "            # 读取指定片段\n",
    "            chunk_data, _ = sf.read(\n",
    "                audio_path,\n",
    "                start=original_start_sample,\n",
    "                stop=original_end_sample,\n",
    "                dtype='float32'  # 建议使用float32以保持精度\n",
    "            )\n",
    "\n",
    "            # 构造输出文件名\n",
    "            output_filename = os.path.join(out_dir, f\"chunk_{i+1:03d}.wav\")\n",
    "            \n",
    "            # 将切分出的音频块保存为新文件，使用原始采样率\n",
    "            sf.write(output_filename, chunk_data, original_sr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  处理切片 {i+1:03d} 时发生错误: {e}\\n\")\n",
    "\n",
    "split_audio_by_vad(\n",
    "    timestamps=merged_segments, \n",
    "    audio_path=uvr_result, \n",
    "    vad_sr=VAD_SAMPLING_RATE, \n",
    "    out_dir=\"./temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055c56b",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86114d93",
   "metadata": {},
   "source": [
    "### 多模态LLM语音识别（二选一）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f3f04",
   "metadata": {},
   "source": [
    "##### Gemma-3n语音识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemma-3n-E4B-it，需要transformers>=4.53.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993e112",
   "metadata": {},
   "source": [
    "##### Phi-4语音识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi-4-multimodal-instruct，需要transformers==4.51.3，否则可能报错AttributeError: 'Phi4MMModel' object has no attribute 'prepare_inputs_for_generation'，详见https://huggingface.co/microsoft/Phi-4-multimodal-instruct/discussions/75#684aea27aef16192b9f52104\n",
    "# !uv pip install transformers==4.51.3\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "model_path = './Phi-4-multimodal-instruct'\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # _attn_implementation='flash_attention_2',\n",
    "    _attn_implementation='sdpa', #,'eager'\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3369a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_path, 'generation_config.json')\n",
    "# print(generation_config)\n",
    "user_prompt = '<|user|>'\n",
    "assistant_prompt = '<|assistant|>'\n",
    "prompt_suffix = '<|end|>'\n",
    "\n",
    "# speech_prompt = \"将这段日语语音转写为文本，直接输出文本内容和标点符号，不要输出任何其他内容\"\n",
    "speech_prompt = \"Transcribe this Japanese speech into text, ONLY output the text content with punctuation, DO NOT output anything else\"\n",
    "prompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\n",
    "\n",
    "# 遍历temp文件夹\n",
    "folder_path='./temp'\n",
    "all_files = os.listdir(folder_path)\n",
    "transcribe_result = []\n",
    "for filename in all_files:\n",
    "    torch.cuda.empty_cache()\n",
    "    # 检查文件名是否以 'chunk_' 开头\n",
    "    if filename.startswith('chunk_'):\n",
    "        # 构建完整的文件路径\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "        # 额外检查是否是文件而不是目录\n",
    "        if os.path.isfile(full_path):\n",
    "            try:\n",
    "                audio_data, sample_rate = sf.read(full_path)\n",
    "                audio = (audio_data, sample_rate)\n",
    "                inputs = processor(text=prompt, audios=[audio], return_tensors='pt').to('cuda')\n",
    "\n",
    "                generate_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    num_beams=5,\n",
    "                    max_new_tokens=128,\n",
    "                    generation_config=generation_config,\n",
    "                    num_logits_to_keep=1, # NoneType Slicing Error in `modeling_phi4mm.py`: “bad operand type for unary -”，详见https://huggingface.co/microsoft/Phi-4-multimodal-instruct/discussions/46#67deef41a13e68cb45a8c2c2\n",
    "                )\n",
    "\n",
    "                generate_ids = generate_ids[:, inputs['input_ids'].shape[1] :]\n",
    "\n",
    "                response = processor.batch_decode(\n",
    "                    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "                )[0]\n",
    "                transcribe_result.append(response)\n",
    "                # print(response)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14056454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卸载LLM\n",
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680702e8",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6e226",
   "metadata": {},
   "source": [
    "### 生成字幕文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安全检查：确保两个列表长度相同\n",
    "if len(merged_segments) != len(transcribe_result):\n",
    "    print(\"错误：时间戳片段和转写结果的数量不匹配！\")\n",
    "else:\n",
    "    # 使用 zip 并行遍历\n",
    "    for segment, text in zip(merged_segments, transcribe_result):\n",
    "        segment['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样点转换为时间\n",
    "\n",
    "def convert_to_time(sample_index: int, sampling_rate: int) -> str:\n",
    "    \"\"\"\n",
    "    Converts an audio sample index to an SRT timestamp string HH:MM:SS,ms.\n",
    "\n",
    "    Args:\n",
    "        sample_index (int): The sample index from the VAD output.\n",
    "        sampling_rate (int): The sampling rate of the audio (e.g., 16000).\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted SRT timestamp.\n",
    "    \"\"\"\n",
    "    # 计算总毫秒数，使用 round 确保精度\n",
    "    total_milliseconds = round(sample_index / sampling_rate * 1000)\n",
    "\n",
    "    # 分解为小时、分钟、秒和毫秒\n",
    "    milliseconds = total_milliseconds % 1000\n",
    "    total_seconds = total_milliseconds // 1000\n",
    "    \n",
    "    seconds = total_seconds % 60\n",
    "    total_minutes = total_seconds // 60\n",
    "    \n",
    "    minutes = total_minutes % 60\n",
    "    hours = total_minutes // 60\n",
    "\n",
    "    # 使用 f-string 格式化为 HH:MM:SS,ms\n",
    "    # :02d 表示整数，宽度为2，不足用0填充\n",
    "    # :03d 表示整数，宽度为3，不足用0填充\n",
    "    srt_time = f\"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
    "    # ffmpeg_time = f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
    "\n",
    "    return srt_time # , ffmpeg_time\n",
    "\n",
    "\n",
    "with open(f'{source_file_name}.srt', 'w', encoding='utf-8') as f:\n",
    "    # 字幕文件会生成在当前目录下\n",
    "    block_index = 1\n",
    "    start_times_srt, end_times_srt = [], []\n",
    "\n",
    "    for segment in merged_segments:\n",
    "        # 使用转换函数获取格式化的开始和结束时间\n",
    "        start_time_srt = convert_to_time(segment['start'], VAD_SAMPLING_RATE)\n",
    "        end_time_srt = convert_to_time(segment['end'], VAD_SAMPLING_RATE)\n",
    "    \n",
    "        # 打印完整的SRT块\n",
    "        f.write(f\"{block_index}\"+\"\\n\")\n",
    "        f.write(f\"{start_time_srt} --> {end_time_srt}\"+\"\\n\")\n",
    "        f.write(segment['text']+\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        block_index += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
